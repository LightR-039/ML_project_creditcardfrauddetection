# ğŸ›¡ï¸ Credit Card Fraud Detection

**Machine Learning & Deep Learning End-to-End Pipeline**

<p align="left">
  <img src="https://img.shields.io/badge/Python-3.10-blue?logo=python" />
  <img src="https://img.shields.io/badge/Notebook-Jupyter-orange?logo=jupyter" />
  <img src="https://img.shields.io/badge/ML-Sklearn-green?logo=scikitlearn" />
  <img src="https://img.shields.io/badge/DL-TensorFlow-yellow?logo=tensorflow" />
  <img src="https://img.shields.io/badge/LightGBM-3.3.0-brightgreen?logo=leaflet" />
  <img src="https://img.shields.io/badge/Status-Completed-success" />
</p>

---

## ğŸš¨ Problem Statement

Credit card fraud poses a major threat to financial institutions, resulting in:

* Direct monetary losses
* Regulatory risk
* Customer dissatisfaction

However, while detecting fraud is essential, **false alarms (false positives)** are even more damaging:

* Legitimate transactions get declined âŒ
* Customers abandon the card (churn) ğŸš¶
* Loss of trust and service fee revenue ğŸ’¸

ğŸ‘‰ **The goal of this project is not only to detect fraud but to minimize false alarms.**
In real-life credit card operations, **reducing customer friction sometimes matters more than catching every fraud case**.

This project builds a balanced fraud detection system optimized for **high recall** (catch fraud) and **low false positives** (avoid blocking customers).

---

## ğŸ“¦ Overview

This repository contains a full end-to-end fraud detection pipeline using the **Kaggle ULB dataset**, including:

* ğŸ” Exploratory Data Analysis
* ğŸ§¼ Preprocessing, scaling, and SMOTE
* ğŸ¤– Baseline & advanced ML models
* ğŸ§  Deep learning MLP models
* ğŸ“Š Excel-based logistic regression (explainability)
* ğŸ“ˆ Model comparison
* ğŸ§¾ Business impact interpretation
* ğŸš§ Limitations & future work
---

## ğŸ“‚ Project Structure

```
fraud-detection/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€README.md                        
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ fraud_detection_full_pipeline.ipynb
â”‚
â”œâ”€â”€ excel/
â”‚   â”œâ”€â”€ LR_excel.xlsx
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€feature_importance/      
â”‚   â””â”€â”€ models/         
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---
# ğŸ“Š Dataset

**Source:** [https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

* **Rows:** 284,807 transactions
* **Fraud cases:** 492 (0.172%)
* **Features:** PCA components (V1â€“V28), *Time*, *Amount*
* **Target:**

  * `0` â€” normal
  * `1` â€” fraud

It is **highly imbalanced**, requiring special handling through:

* SMOTE
* Class weights
* Threshold tuning

---

# ğŸ” Exploratory Data Analysis (EDA)

Includes:

* Class distribution visualization
* Transaction amount patterns
* Correlation heatmaps
* PCA component distribution
* Outlier analysis

---

# ğŸ§¼ Preprocessing

* Remove unnecessary columns
* Stratified train-test split
* Scale **Time** and **Amount** using `StandardScaler`
* Apply **SMOTE only on training set** (Logistic Regression only)
* Export scaled + SMOTE datasets to Excel

---

# ğŸ¤– Machine Learning Models

### 1ï¸âƒ£ Logistic Regression (with SMOTE + threshold tuning)

â­ Most interpretable & best suited for regulated financial environments.

### 2ï¸âƒ£ Random Forest

âœ” Handles non-linear features
âœ” Built-in feature importance

### 3ï¸âƒ£ LightGBM

âš¡ Fast and highly accurate
âœ” Handles imbalance with built-in parameters

---

# ğŸ§  Deep Learning Models

### 4ï¸âƒ£ Shallow MLP

* 1â€“2 dense layers
* Fast and simple baseline

### 5ï¸âƒ£ Deep MLP

* Multiple dense layers
* More learning capacity

---

# ğŸ† Model Performance Summary

*(Replace these placeholders with your final numbers.)*

| Model                        | ROC-AUC   | PR-AUC   | Recall (Fraud) | False Alarm Rate | Notes                        |
| ---------------------------- | --------- | -------- | -------------- | ---------------- | ---------------------------- |
| **Logistic (SMOTE + tuned)** | **0.973** | **0.71** | **0.82**       | **Very Low**     | Best business-friendly model |
| LightGBM                     | 0.98      | 0.70     | 0.80           | Low              | High performance             |
| Deep MLP                     | 0.98      | 0.72     | 0.81           | Medium-Low       | Strong nonlinear learner     |
| Shallow MLP                  | 0.97      | 0.68     | 0.78           | Medium           | Good baseline                |
| Random Forest                | 0.96      | 0.65     | 0.75           | Medium-High      | Overfits slightly            |

---

# ğŸ“ˆ Business Interpretation

### âœ” Catch as much fraud as possible

Fraud is costly â€” every undetected fraud is a direct loss.

### âœ” But false alarms cost even more

False positives cause:

* Customer dissatisfaction
* Lost revenue
* Declined transactions
* Customer churn

Banks often accept a small amount of fraud to maintain good customer experience.

This project focuses on **tuning probability thresholds** to find the optimal business trade-off.

---

# ğŸ§¾ Explainability (Excel Version)

To meet audit & regulatory requirements:

* Export preprocessed + SMOTE training data to Excel
* Train Logistic Regression in Excel using RegressIt
* Compare:

  * Coefficients
  * Odds ratios
  * Feature contributions
  * Probability threshold

This ensures transparent, regulator-friendly modeling.

---

# âš ï¸ Limitations

* Dataset uses PCA-transformed data â†’ limits interpretability
* SMOTE may introduce synthetic noise
* Fraud patterns change over time (concept drift)
* No cost-sensitive learning (fraud cost â‰  false alarm cost)
* Deep learning not optimized for latency in real-time credit systems

---

# ğŸš€ Future Work

* Use raw non-PCA features for better interpretability
* Add SHAP for full explainability
* Cost-sensitive loss functions
* Deploy real-time API (FastAPI + Docker)
* Try anomaly detection methods
* Use sequential models (LSTM, Transformers)

---

# ğŸ›  Installation

```bash
pip install -r requirements.txt
```

---

# â–¶ï¸ Running the Pipeline

Open the main notebook:

```
notebook/fraud_detection_full.ipynb
```



---

# ğŸ™Œ Acknowledgements

Dataset by **UniversitÃ© Libre de Bruxelles (ULB)** Machine Learning Group.


